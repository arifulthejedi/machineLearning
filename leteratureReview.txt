//intruduction of OPTICS
OPTICS clustering is a popular unsupervised machine learning technique used for clustering and visualizing high-dimensional data.it is well-liked density-based clustering algorithm. It generates sorted data points and records each point's reachability and core distances.The OPTICS clustering algorithm has the benefit of being able to handle datasets with a variety of cluster densities and forms, which makes it helpful in a number of applications, including anomaly detection, text clustering, and image segmentation.


One advantage of the optics clustering algorithm is that it can handle datasets with a wide range of cluster densities and shapes, making it useful in a variety of applications, such as image segmentation, text clustering, and anomaly detection.

It is especially helpful when the number of clusters is uncertain or when the data contains noise or changing densities. You can select the clustering fineness with more freedom using OPTICS hierarchical clustering tree, which also makes it possible to see the clustering structure in all of its detail. Overall, the OPTICS method is a robust and versatile clustering algorithm that can handle a variety of real-world datasets, making it a crucial tool for applications in data analysis and machine learning.

//which field this technique is useful
In many situations where clustering is required, the OPTICS algorithm might be helpful. Such as:

1.Finding anomalies or outliers in datasets: OPTICS can be used to locate anomalies or outliers by locating points that do not fit into any cluster.

2.Image segmentation: OPTICS can segment images by grouping pixels according to their color or texture characteristics.

3.Social network analysis: OPTICS can be used to identify communities or groups of individuals in social networks based on their interactions or behavior.

4.Segmenting consumers: OPTICS can be used to group clients based on their buying patterns, demographics, or other characteristics to pinpoint groups with comparable requirements or preferences.

5.Monitoring of the environment: OPTICS can be used to group environmental data, such as measurements of the quality of the air or water, to pinpoint regions with comparable environmental conditions.

6.Analysis of criminal behavior patterns: OPTICS can be used to group criminal activity data in order to find hotspots or patterns.

These are just a few instances of how OPTICS can be used in numerous fields. The technique is an effective tool for grouping enormous and complicated datasets because of its adaptability and scalability.




There are several existing methods for implementing the OPTICS clustering algorithm. In this article the orginial OPTICS algorithm (proposed Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel, and Jorg Sander in 1999) methode is choosen.Although it has a number of drawbacks that can limit its performance in some circumstances.
 
Some of these limitations include:

1.Computing complexity: For big datasets, the original OPTICS approach might be computationally expensive due to the need to generate the reachability graph effectively using a priority queue. For some applications, this might reduce the algorithm's ability to scale.


2.Memory consumption: The original OPTICS algorithm's priority queue can require a lot of memory for large datasets, which may limit its suitability in environments with memory restrictions.


3.Sensitivity to parameter settings: The original OPTICS algorithm needs a number of parameters to be specified, including the neighborhood size parameter (epsilon) and the minimum number of points in a cluster. The selection of these parameters may have an impact on the algorithm's performance, necessitating manual adjustment or the use of heuristics.


4.Limited support for high-dimensional data: Because the paired distances between each data point must be calculated, the original OPTICS algorithm is susceptible to the dimensionality curse. For high-dimensional data, this may be computationally demanding and memory-intensive.


//intruduction of aggolomerative
Agglomerative clustering is a machine learning and data analysis method for hierarchical clustering. It comprises clustering comparable data points into clusters, merging these clusters from the bottom up, and repeating this process until all the points are a member of the same cluster.

The advantage of developing a hierarchical structure of clusters using a dendrogram is one advantage of agglomerative clustering. This structure provides a visual representation of the connections between clusters and makes it easier to identify natural groupings within the data.

//which field this technique is useful
Agglomerative clustering is effective in many applications where it is useful to group related things together. The following are some scenarios in which agglomerative clustering can be effective:

1.Marketing: Customers can be categorized using agglomerative clustering based on their purchasing patterns, demographics, or other variables. This can assist companies in identifying target client segments for particular goods or services and developing tailored marketing plans.

2.Image segmentation: Agglomerative clustering can be used to segment an image by gathering pixels that are part of the same object or location. For tasks like object detection, background removal, or image compression, this can be helpful.

3.Genomics: Genes or proteins can be grouped via agglomerative clustering depending on their expression profiles, sequence information, or other variables. Researchers may be able to learn more about biological processes and mechanisms and uncover shared genes or functional modules with the use of this.

4.Social Network: Agglomerative clustering can be used in social network analysis to classify users or nodes in a network according to their interactions, shared interests, or other criteria. This can aid researchers in locating communities, influential individuals, or network-wide trends.

5.Finance: Stocks or assets can be grouped using aggregative clustering based on their correlation or covariance structure. This can assist investors in managing risks, expanding their portfolios, or finding arbitrage possibilities.

These are just a handful of the applications where agglomerative clustering can be useful. Agglomerative clustering can be generally helpful in any application where it is desirable to group similar objects together and if the data has a natural hierarchical structure.


Agglomerative clustering can be implemented using a variety of methodes.Which agglomerative clustering method is employed will depend on the type of data being clustered and the precise objectives of the study. This article will focus on "complete linkage" and "single linkage" in particular. Both methodes has their own drawbacks.

Single linkage:
Single linkage clustering has a number of problems, including a sensitivity to noise and outliers in the data. A single outlier or noise point close to a cluster can have a significant impact on the distance between that cluster and other clusters because the distance between two clusters in single linkage is based on the shortest distance between any two points in the clusters. This could result in subpar clustering results.


Complete linkage:
For large datasets, complete linkage clustering can be computationally expensive since it determines the greatest possible distance between every pair of points in the clusters being merged. Applying complete linkage clustering to datasets with a lot of data points or dimensions might be difficult because of this.


Data cleansing is a crucial step to take before applying any clustering to a dataset in order to get optimal results.  Here are some steps for data cleaning specifically for clustering. 

1.Eliminate missing or incomplete data: Missing or incomplete data can lead to incorrect or biased clusters. It is essential to delete or impute missing data before grouping.

2.Eliminate unnecessary variables:Including unnecessary variables can result in false or noisy clusters. Removing variables that are irrelevant to the clustering analysis is a good approach.

3.Normalize or standardize the data: It is necessary to normalize or standardize the variables before clustering if the dataset contains variables with different scales or units. This guarantees that each variable in the clustering analysis is given the same weight.

4.Identify and remove outliers: The outcomes of a clustering analysis can be greatly impacted by outliers. Before clustering, it's critical to locate and eliminate outliers; alternatively, think about employing a strong distance metric that is less susceptible to outliers.

5.Choose the appropriate distance metric: The clustering outcomes can be influenced by the distance metric selection. It is crucial to select a distance metric that works with the data being studied and the clustering goals.

6.Analyze the data's quality: Before clustering, carefully evaluate the data's quality. This entails looking for data entry mistakes, consistency problems, and other problems that can have an impact on the precision and dependability of the clustering results.


//Litereture review
# Agglomerative

Here are some general steps to follow to implement the agglomerative hierarchical clustering algorithm for the data set:

1.Select the best linkage technique: Select a linkage technique that is suitable for the type of data being studied and the clustering goals. Two popular techniques are complete linkage and single linkage.


2.Determine how many clusters are there: Choose how many clusters you want to use in the analysis. This could be decided upon using exploratory analysis or existing information.

3.Select the distance metric: Select a suitable distance metric to determine how similar the data points are to one another. Euclidean distance, Manhattan distance, and cosine distance are three often used distance measures.

4.Make a distance matrix: Calcalculating the pairwise distances between all of the data points.

5.Cluster initialization: Every data point is originally assigned to a separate cluster.

6.Clusters merging: Using the specified linkage method, combine the two nearest clusters, then update the distance matrix.

7.Repeat steps 5 and 6: Repeat steps 5 and 6 until the necessary number of clusters is obtained. This is done by combining the two closest clusters.

8.Visualize the results of the clustering:  Analysis, and then analyze the clusters in light of the information and the clustering goals.

9.Evaluate the outcomes: Utilize relevant metrics to measure the quality of the clustering results, such as the silhouette score, cluster purity, or within-cluster sum of squares.

In general, agglomerative clustering implementation entails selecting appropriate parameters, prepping the data, and performing iterative cluster merging based on the selected linking mechanism. The analysis's result can be utilized for a number of things, including pattern recognition, gathering related data points, and prediction-making.

Here are some visual examples of dummy data set on t-SNE plot.

//insert the all tsne plot pic

Here are the outcomes of using the agglomerative algorithm for the data set. There are many ways to evaluate the label's accuracy. However, Adjusted Rand Index (ARI) is used in this case.

//show the result table with single and complete linkage



Use single linkage to find compact, long clusters or clusters with irregular shapes when the data points inside each cluster are generally dense and well separated from one another. The single linkage is sensitive to noise and outliers, hence it could not perform well when there is a lot of noise in the data or when there are outliers.

On the other hand apply complete linkage to identify small, circular groups or clusters with similar sizes and shapes when the data points within each cluster are spread out and less well isolated from one another. Complete linkage tends to form clusters with a bias towards equal variances, although it is less vulnerable to noise and outliers than single linkage.


# OPtics
The OPTICS method can be implemented in a variety of ways depending on the programming language and the particular library or package being used. Here is an overview of the procedures necessary to install OPTICS:

1.Compute the reachability distance: The OPTICS method calculates the reachability distance for each point in the dataset to each other point in the dataset. The minimal distance needed to get from one location to another along a path of decreasing reachability is known as the reachability distance.

2.Create the reachability graph: The reachability graph is a directed graph with nodes that represent the data points and edges that reflect the separations between them that are within the reach of those points. A directed edge is added between two points if the reachability distance between them is smaller than a given radius.

3.Make the reachability plot: The reachability plot captures the local density structure of the data and is a 1D representation of the reachability graph. The points are plotted against their index after being ordered according to their reachability distance.

4.Determine cluster hierarchies: The optimal cluster number and cluster hierarchies can be determined using the reachability plot. To do this, the minimum cluster size must be determined as well as any sharp drops in reachability distances that correlate to cluster boundaries.

5.Cluster extraction method: Based on the minimal cluster size and the cluster hierarchy, the clusters are retrieved from the reachability graph. A set of clusters and their related core points with a minimum reachability distance larger than the minimum cluster size are the end result.

The OPTICS algorithm has several parameters that can affect the clustering results. Here are some essential features that should be properly set and considered during implementation.

1.`min_sample`: The parameter "min_samples" determines the bare minimum of samples needed to create a dense zone. More clusters will come from a smaller value, while fewer clusters will result from a bigger value. The dataset and intended cluster granularity must be considered while selecting the appropriate value.

2.`xi`: The minimal distance between two cluster hierarchies is controlled by the 'xi' parameter. Clusters will increase with a lesser value while decreasing with a bigger value. This value is crucial for datasets with different densities.

3.`metric`: The distance metric used to calculate the pairwise reachability distance is specified by the option "metric." It is crucial to select a distance measure that is appropriate for the dataset and the issue at hand because it might have a major impact on the clustering outcomes.

4.'eps': This specifies the radius of the area surrounding each point. The reachability graph has edges connecting points within this radius, which are connected via the concept of neighbors. The density and shape of the dataset will determine the appropriate value for eps.

5.`min_cluster_size`:  The `min_cluster_size` parameter determines the bare minimum of points needed to construct a cluster. Outliers are points that do not belong to any cluster. The intended cluster granularity and dataset size must be taken into consideration when determining the appropriate value for `min_cluster_size`.

To get the appropriate clustering results, it's necessary to carefully select these parameters. To choose the best parameters for the dataset and the current issue, experiment with various parameter values and visualize the outcomes.

The following displays the accuracy results of the OPTICS algorithm's output for certain data sets.

//showing the list of optics accuracy




OPTICS can handle datasets of any size and shape, and it is suited for grouped datasets with varied densities and shapes. On datasets with uniform densities, it might not work as well because there might not be sufficient variation in the reachability distances to correctly identify clusters.


In general, it is a good idea for trying out various clustering algorithms to see which one is most effective for a specific dataset.















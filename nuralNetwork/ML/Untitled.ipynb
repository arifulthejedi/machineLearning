{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7464fca6-dc58-4f5b-8ede-d0b795ecc17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 969ms/step - accuracy: 0.0300 - loss: 2.8156 - val_accuracy: 0.5000 - val_loss: 0.7822\n",
      "Epoch 2/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 474ms/step - accuracy: 0.4896 - loss: 1.0104 - val_accuracy: 0.5000 - val_loss: 0.8569\n",
      "Epoch 3/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 468ms/step - accuracy: 0.4896 - loss: 1.0035 - val_accuracy: 0.5000 - val_loss: 0.7053\n",
      "Epoch 4/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 524ms/step - accuracy: 0.4896 - loss: 0.9160 - val_accuracy: 0.5000 - val_loss: 0.5632\n",
      "Epoch 5/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 459ms/step - accuracy: 0.4896 - loss: 0.7464 - val_accuracy: 0.5000 - val_loss: 0.5234\n",
      "Epoch 6/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 447ms/step - accuracy: 0.4896 - loss: 0.6263 - val_accuracy: 0.5000 - val_loss: 0.5041\n",
      "Epoch 7/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 484ms/step - accuracy: 0.4896 - loss: 0.6325 - val_accuracy: 0.5000 - val_loss: 0.4724\n",
      "Epoch 8/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 496ms/step - accuracy: 0.4896 - loss: 0.6146 - val_accuracy: 0.6471 - val_loss: 0.4212\n",
      "Epoch 9/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 466ms/step - accuracy: 0.6397 - loss: 0.5147 - val_accuracy: 0.6471 - val_loss: 0.3812\n",
      "Epoch 10/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 482ms/step - accuracy: 0.6397 - loss: 0.4729 - val_accuracy: 0.6765 - val_loss: 0.3465\n",
      "Epoch 11/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 439ms/step - accuracy: 0.6697 - loss: 0.4462 - val_accuracy: 0.7353 - val_loss: 0.3152\n",
      "Epoch 12/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 565ms/step - accuracy: 0.7298 - loss: 0.3904 - val_accuracy: 0.7059 - val_loss: 0.2901\n",
      "Epoch 13/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 471ms/step - accuracy: 0.6998 - loss: 0.3805 - val_accuracy: 0.7647 - val_loss: 0.2578\n",
      "Epoch 14/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 869ms/step - accuracy: 0.7598 - loss: 0.3275 - val_accuracy: 0.7941 - val_loss: 0.2448\n",
      "Epoch 15/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 508ms/step - accuracy: 0.7898 - loss: 0.3284 - val_accuracy: 0.6471 - val_loss: 0.3094\n",
      "Epoch 16/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 484ms/step - accuracy: 0.6397 - loss: 0.3832 - val_accuracy: 0.5882 - val_loss: 0.4453\n",
      "Epoch 17/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 483ms/step - accuracy: 0.5797 - loss: 0.5952 - val_accuracy: 0.7353 - val_loss: 0.2318\n",
      "Epoch 18/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 464ms/step - accuracy: 0.7298 - loss: 0.3015 - val_accuracy: 0.7647 - val_loss: 0.2199\n",
      "Epoch 19/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 718ms/step - accuracy: 0.7598 - loss: 0.2780 - val_accuracy: 0.8529 - val_loss: 0.2077\n",
      "Epoch 20/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 665ms/step - accuracy: 0.8499 - loss: 0.2757 - val_accuracy: 0.8824 - val_loss: 0.1713\n",
      "Epoch 21/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 554ms/step - accuracy: 0.8799 - loss: 0.2276 - val_accuracy: 0.9118 - val_loss: 0.1489\n",
      "Epoch 22/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 695ms/step - accuracy: 0.9099 - loss: 0.1942 - val_accuracy: 0.9118 - val_loss: 0.1465\n",
      "Epoch 23/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 551ms/step - accuracy: 0.9099 - loss: 0.1884 - val_accuracy: 1.0000 - val_loss: 0.1208\n",
      "Epoch 24/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 585ms/step - accuracy: 1.0000 - loss: 0.1574 - val_accuracy: 1.0000 - val_loss: 0.1102\n",
      "Epoch 25/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 523ms/step - accuracy: 1.0000 - loss: 0.1455 - val_accuracy: 1.0000 - val_loss: 0.0862\n",
      "Epoch 26/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 516ms/step - accuracy: 1.0000 - loss: 0.1144 - val_accuracy: 0.9118 - val_loss: 0.0933\n",
      "Epoch 27/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 508ms/step - accuracy: 0.9099 - loss: 0.1206 - val_accuracy: 1.0000 - val_loss: 0.0749\n",
      "Epoch 28/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 472ms/step - accuracy: 1.0000 - loss: 0.0997 - val_accuracy: 1.0000 - val_loss: 0.0475\n",
      "Epoch 29/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 702ms/step - accuracy: 1.0000 - loss: 0.0632 - val_accuracy: 0.9706 - val_loss: 0.0524\n",
      "Epoch 30/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 702ms/step - accuracy: 0.9700 - loss: 0.0686 - val_accuracy: 1.0000 - val_loss: 0.0296\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0392 \n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def show_image_from_array(image_array, cmap='gray'):\n",
    "    plt.imshow(image_array, cmap=cmap)\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "def load_images_and_labels(directory, target_size=(100, 100)):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through each subdirectory (assuming each folder corresponds to a label)\n",
    "    for label_name in os.listdir(directory):\n",
    "        folder_path = os.path.join(directory, label_name)\n",
    "\n",
    "        # Check if it's a directory\n",
    "        if os.path.isdir(folder_path):\n",
    "            # Iterate through each image file in the folder\n",
    "            for filename in os.listdir(folder_path):\n",
    "                image_path = os.path.join(folder_path, filename)\n",
    "                try:\n",
    "                    # Open image using PIL\n",
    "                    with Image.open(image_path) as img:\n",
    "                        # Resize image\n",
    "                        img = img.resize(target_size)\n",
    "                        # Convert image to array\n",
    "                        images.append(np.array(img))\n",
    "                        labels.append(label_name)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"trainData\"\n",
    "images, labels = load_images_and_labels(directory_path,(100,100))\n",
    "\n",
    "#label inversion in to integer array\n",
    "labels = np.array(labels)\n",
    "labels[labels == \"healthy\"] = 0\n",
    "labels[labels == \"infected\"] = 1\n",
    "labels = [int(num_str) for num_str in labels]\n",
    "\n",
    "\n",
    "#image reshaping\n",
    "images = np.array(images)/255\n",
    "\n",
    "\n",
    "\n",
    "#cnn mideling\n",
    "\n",
    "# model2 = tf.keras.Sequential([\n",
    "#     #cnn layer\n",
    "#     tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3),activation = 'relu',input_shape=(150,150,4)),\n",
    "#     tf.keras.layers.MaxPool2D(2,2),\n",
    "    \n",
    "#     #layer 2\n",
    "#     tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3),activation = 'relu'),\n",
    "#     tf.keras.layers.MaxPool2D(2,2),\n",
    "    \n",
    "#     tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "\n",
    "#     #dense layer\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(64,activation=\"relu\"),\n",
    "#     tf.keras.layers.Dense(10,activation=\"softmax\") #output layer  \n",
    "# ])\n",
    "\n",
    "model2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 4)), # Assuming input images are 100x100 pixels with 4 channels\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#model2.summary()\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((images,labels)).batch(32)\n",
    "\n",
    "\n",
    "# #train the model\n",
    "model2.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,   #iteration over the model to predict correct\n",
    "    validation_data = train_dataset\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model2.evaluate(train_dataset)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef901e94-6ed7-4180-abe5-e86e3d696ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "0 = healthy 1 = infected\n",
      "prediction: 0\n"
     ]
    }
   ],
   "source": [
    "#load image\n",
    "image_path = \"testImg2.png\"\n",
    "image = Image.open(image_path)\n",
    "image = image.resize((100, 100))  # Resize the image to match the input shape of the model\n",
    "image = np.array(image) / 255.0  # Normalize pixel values\n",
    "\n",
    "# Add an extra dimension to match the input shape expected by the model\n",
    "image = np.expand_dims(image, axis=0)  # Shape: (1, 100, 100, 4)\n",
    "\n",
    "\n",
    "predict = model2.predict(image)\n",
    "predicted_class = np.argmax(predict)\n",
    "\n",
    "print(\"0 = healthy 1 = infected\")\n",
    "print(\"prediction:\",predicted_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a508e5b-656c-42ea-95ef-14b1e194ae1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
